{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/external-libraries/')\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_root = './raw_data/新闻描述二期第一批数据/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = [file for file in os.listdir(annotation_root) if file[-4:] == '.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MBM0W5J4M8XP_cm新闻标注二期—游行.txt',\n",
       " 'V2X4HARUIC28_cm新闻标注二期—火灾.txt',\n",
       " 'W60JBBZY0433_cm新闻标注二期—地震.txt',\n",
       " '7CBDH771T2XH_cm新闻标注二期—空难.txt',\n",
       " 'NY5SRX7Y3THH_cm新闻标注二期—暴乱.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = dict_keys(['response', 'metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_data(data_root,filename):\n",
    "    entity = {}\n",
    "    sents = []\n",
    "    sents_tokenize = []\n",
    "    with open(os.path.join(data_root,filename),'r') as f:\n",
    "        data = json.load(f)\n",
    "    for data_part in data['response']['annotations']:\n",
    "        if len(data_part['attributes']) > 5:\n",
    "            seg_list = jieba.cut(data_part['attributes'].strip().replace(u'。',''),cut_all = False)\n",
    "            sents_tokenize.append(list(seg_list))\n",
    "            sents.append(data_part['attributes'])\n",
    "    img_name = annotation.split('_')[0]\n",
    "    if sents != []:\n",
    "        entity['image_name'] = img_name + '.jpg'\n",
    "        entity['sents'] = sents\n",
    "        entity['sents_token'] = sents_tokenize\n",
    "        return entity\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "data_annotations = []\n",
    "for files in annotation_file:\n",
    "    for annotation in os.listdir(os.path.join(annotation_root,files)):\n",
    "        entity = bulid_data(os.path.join(annotation_root,files),annotation)\n",
    "        #print(image_name,label)\n",
    "        if entity:\n",
    "            data_annotations.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5123"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_vocab(imgs):\n",
    "    param = {}\n",
    "    counts = {}\n",
    "    for img in imgs:\n",
    "        #print(img['sents_token'])\n",
    "        for sent in img['sents_token']:\n",
    "            for w in sent:\n",
    "                counts[w] = counts.get(w,0) + 1\n",
    "    cw = sorted([(count,w) for w,count in counts.items()],reverse=True)\n",
    "    print('top words and their counts:')\n",
    "    print('\\n'.join(map(str,cw[:100])))\n",
    "    \n",
    "    #print(cw)\n",
    "    total_words = sum(counts.values())\n",
    "    print('total words:', total_words)\n",
    "    bad_words = [w for w,n in counts.items() if n <= 1]\n",
    "    vocab = [w for w,n in counts.items() if n > 1]\n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    print('number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "    print('number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print('number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))\n",
    "    sent_length = {}\n",
    "    for img in imgs:\n",
    "        for sent in img['sents_token']:\n",
    "            nw = len(sent)\n",
    "            sent_length[nw] = sent_length.get(nw,0) +1\n",
    "    max_len = max(sent_length.keys())\n",
    "    param['max_length'] = max_len\n",
    "    print('max length sentence in raw data: ', max_len)\n",
    "    print('sentence length distribution (count, number of words):')\n",
    "    sum_len = sum(sent_length.values())\n",
    "    for i in range(max_len+1):\n",
    "        print('%2d: %10d   %f%%' % (i, sent_length.get(i,0), sent_length.get(i,0)*100.0/sum_len))\n",
    "        \n",
    "\n",
    "    if bad_count > 0:\n",
    "        # additional special UNK token we will use below to map infrequent words to\n",
    "        print('inserting the special UNK token')\n",
    "        vocab.append(u'UNK')\n",
    "    #print(vocab)\n",
    "    for img in imgs:\n",
    "        img['final_captions'] = []\n",
    "        for sent in img['sents_token']:\n",
    "            caption = [w if counts.get(w,0) > 1 else u'UNK' for w in sent]\n",
    "            img['final_captions'].append(caption)\n",
    "    return vocab,param\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words and their counts:\n",
      "(12816, '，')\n",
      "(8200, '发生')\n",
      "(6882, '某地')\n",
      "(3788, '现场')\n",
      "(3126, '某')\n",
      "(3003, '火灾')\n",
      "(2708, '在')\n",
      "(2327, '地震')\n",
      "(2251, '的')\n",
      "(2132, '游行')\n",
      "(1941, '地区')\n",
      "(1693, '暴乱')\n",
      "(1683, '救援')\n",
      "(1595, '群众')\n",
      "(1563, '正在')\n",
      "(1164, '建筑')\n",
      "(1066, '浓烟')\n",
      "(1060, '抗议')\n",
      "(983, '被')\n",
      "(967, '警察')\n",
      "(952, '进行')\n",
      "(825, '严重')\n",
      "(804, '举行')\n",
      "(744, '房屋')\n",
      "(732, '一名')\n",
      "(731, '人员')\n",
      "(714, '事件')\n",
      "(685, '街头')\n",
      "(684, '工作')\n",
      "(682, '消防员')\n",
      "(681, '多名')\n",
      "(655, '上')\n",
      "(632, '废墟')\n",
      "(617, '大火')\n",
      "(596, '着')\n",
      "(571, '事故现场')\n",
      "(549, '大量')\n",
      "(532, '活动')\n",
      "(514, '了')\n",
      "(478, '失火')\n",
      "(477, '滚滚')\n",
      "(463, '抗议者')\n",
      "(438, '汽车')\n",
      "(433, '手持')\n",
      "(427, '节日')\n",
      "(413, '有')\n",
      "(390, '聚集')\n",
      "(387, '一')\n",
      "(362, '建筑物')\n",
      "(361, '示威者')\n",
      "(361, '后')\n",
      "(347, '飞机')\n",
      "(337, '高举')\n",
      "(337, '骚乱')\n",
      "(331, '袭击')\n",
      "(325, '男子')\n",
      "(324, '旗帜')\n",
      "(314, '火势')\n",
      "(313, '围观')\n",
      "(309, '损坏')\n",
      "(305, '众多')\n",
      "(303, '残骸')\n",
      "(302, '遭受')\n",
      "(301, '示威游行')\n",
      "(300, '示威')\n",
      "(293, '一片')\n",
      "(278, '受损')\n",
      "(274, '从')\n",
      "(274, '上空')\n",
      "(273, '森林')\n",
      "(266, '空难')\n",
      "(253, '民众')\n",
      "(252, '标语牌')\n",
      "(248, '中')\n",
      "(246, '展开')\n",
      "(244, '烧毁')\n",
      "(244, '一辆')\n",
      "(243, '凶猛')\n",
      "(240, '庆祝')\n",
      "(238, '吞噬')\n",
      "(237, '笼罩')\n",
      "(236, '天空')\n",
      "(227, '周围')\n",
      "(225, '表演者')\n",
      "(206, '拿')\n",
      "(205, '维持秩序')\n",
      "(204, '站')\n",
      "(200, '浓烟滚滚')\n",
      "(199, '标语')\n",
      "(199, '坍塌')\n",
      "(195, '和')\n",
      "(191, '火光冲天')\n",
      "(187, '灭火')\n",
      "(187, '消防车')\n",
      "(184, '激动')\n",
      "(180, '维护')\n",
      "(180, '情绪')\n",
      "(174, '街道')\n",
      "(172, '前')\n",
      "(171, '表示')\n",
      "total words: 124317\n",
      "number of bad words: 1766/4210 = 41.95%\n",
      "number of words in vocab would be 2444\n",
      "number of UNKs: 1766/124317 = 1.42%\n",
      "max length sentence in raw data:  27\n",
      "sentence length distribution (count, number of words):\n",
      " 0:          0   0.000000%\n",
      " 1:          0   0.000000%\n",
      " 2:          4   0.038990%\n",
      " 3:          5   0.048738%\n",
      " 4:          3   0.029243%\n",
      " 5:          1   0.009748%\n",
      " 6:         34   0.331416%\n",
      " 7:        125   1.218442%\n",
      " 8:        398   3.879520%\n",
      " 9:       1009   9.835267%\n",
      "10:       1533   14.942977%\n",
      "11:       1584   15.440101%\n",
      "12:       1426   13.899990%\n",
      "13:       1293   12.603568%\n",
      "14:        972   9.474608%\n",
      "15:        749   7.300907%\n",
      "16:        438   4.269422%\n",
      "17:        303   2.953504%\n",
      "18:        192   1.871527%\n",
      "19:         91   0.887026%\n",
      "20:         55   0.536115%\n",
      "21:         22   0.214446%\n",
      "22:         10   0.097475%\n",
      "23:          6   0.058485%\n",
      "24:          2   0.019495%\n",
      "25:          2   0.019495%\n",
      "26:          1   0.009748%\n",
      "27:          1   0.009748%\n",
      "inserting the special UNK token\n"
     ]
    }
   ],
   "source": [
    "vocab,param = bulid_vocab(data_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in data_annotations:\n",
    "    n = len(img['final_captions'])\n",
    "    if n==0:\n",
    "        print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "itow = {i+1:w for i,w in enumerate(vocab)} # a 1-indexed vocab translation table\n",
    "wtoi = {w:i+1 for i,w in enumerate(vocab)} # inverse table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(imgs, params, wtoi):\n",
    "    max_length = params['max_length']\n",
    "    N = len(imgs)\n",
    "    M = sum(len(img['final_captions']) for img in imgs) # total number of captions\n",
    "    print(max_length,N,M)\n",
    "    label_arrays = []\n",
    "    label_start_ix = np.zeros(N, dtype='uint32') # note: these will be one-indexed\n",
    "    label_end_ix = np.zeros(N, dtype='uint32')\n",
    "    label_length = np.zeros(M, dtype='uint32')\n",
    "    caption_counter = 0\n",
    "    counter = 1\n",
    "    for i,img in enumerate(imgs):\n",
    "        n = len(img['final_captions'])\n",
    "        assert n > 0, 'error: some image has no captions'\n",
    "        Li = np.zeros((n, max_length), dtype='uint32')\n",
    "        for j,s in enumerate(img['final_captions']):\n",
    "            label_length[caption_counter] = min(max_length, len(s)) # record the length of this sequence\n",
    "            caption_counter += 1\n",
    "            for k,w in enumerate(s):\n",
    "                if k < max_length:\n",
    "                    Li[j,k] = wtoi[w]\n",
    "        label_arrays.append(Li)\n",
    "        label_start_ix[i] = counter\n",
    "        label_end_ix[i] = counter + n - 1\n",
    "\n",
    "        counter += n\n",
    "    L = np.concatenate(label_arrays, axis=0) # put all the labels together\n",
    "    assert L.shape[0] == M, 'lengths don\\'t match? that\\'s weird'\n",
    "    assert np.all(label_length > 0), 'error: some caption had no words?'\n",
    "\n",
    "    print('encoded captions to array of size ', L.shape)\n",
    "    return L, label_start_ix, label_end_ix, label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 5123 10259\n",
      "encoded captions to array of size  (10259, 27)\n"
     ]
    }
   ],
   "source": [
    "L,label_start_ix, label_end_ix,label_length = encode_captions(data_annotations,param,wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     3,     5, ..., 10254, 10256, 10258], dtype=uint32)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_start_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     4,     6, ..., 10255, 10257, 10259], dtype=uint32)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_end_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 12, 25, ..., 13,  9, 14], dtype=uint32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,    2,    3, ...,    0,    0,    0],\n",
       "       [   1,   20,   21, ...,   22,    0,    0],\n",
       "       ...,\n",
       "       [ 171,  858,    2, ...,    0,    0,    0],\n",
       "       [   1,    2, 1776, ...,    0,    0,    0],\n",
       "       [ 171,  275,    2, ...,    0,    0,    0]], dtype=uint32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lb = h5py.File('news_dataset_label.h5', \"w\")\n",
    "f_lb.create_dataset(\"labels\", dtype='uint32', data=L)\n",
    "f_lb.create_dataset(\"label_start_ix\", dtype='uint32', data=label_start_ix)\n",
    "f_lb.create_dataset(\"label_end_ix\", dtype='uint32', data=label_end_ix)\n",
    "f_lb.create_dataset(\"label_length\", dtype='uint32', data=label_length)\n",
    "f_lb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [ 15  24  21]\n",
      "  [ 23  27  28]\n",
      "  [ 22  26  27]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [ 19  28  25]\n",
      "  [ 19  25  24]\n",
      "  [ 23  27  28]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [ 23  32  29]\n",
      "  [ 29  35  34]\n",
      "  [ 31  37  36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 70  73  77]\n",
      "  [ 65  68  72]\n",
      "  [ 65  68  72]\n",
      "  ...\n",
      "  [ 62  62  62]\n",
      "  [ 59  59  59]\n",
      "  [ 58  58  58]]\n",
      "\n",
      " [[ 66  69  73]\n",
      "  [ 61  64  68]\n",
      "  [ 62  65  69]\n",
      "  ...\n",
      "  [ 65  65  65]\n",
      "  [ 63  63  63]\n",
      "  [ 62  62  62]]\n",
      "\n",
      " [[ 76  80  81]\n",
      "  [ 72  76  77]\n",
      "  [ 71  75  76]\n",
      "  ...\n",
      "  [ 69  69  69]\n",
      "  [ 67  67  67]\n",
      "  [ 66  66  66]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "image_root = './images_1'\n",
    "for img in data_annotations:\n",
    "    img['image_path'] = image_root +'/'+ img['image_name']\n",
    "\n",
    "for img in data_annotations:\n",
    "    image = cv2.imread(img['image_path'])\n",
    "    print(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "龙卷风.xlsx\n",
      "游行.xlsx\n",
      "空难.xlsx\n",
      "矿难.xlsx\n",
      "洪水.xlsx\n",
      "泥石流.xlsx\n",
      "火灾.xlsx\n",
      "海啸.xlsx\n",
      "006306.gif\n",
      "山体滑坡.xlsx\n",
      "坍塌.xlsx\n",
      "交通事故.xlsx\n",
      "暴乱.xlsx\n",
      "爆炸.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil,os\n",
    "path = './raw_data/一万张第二批数据/'\n",
    "new_path = './raw_data/images_2'\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for i in range(len(files)):\n",
    "        #print(files[i])\n",
    "        if (files[i][-3:] == 'jpg') or (files[i][-3:] == 'png') or (files[i][-3:] == 'JPG')or (files[i][-4:] == 'jpeg'):\n",
    "            file_path = root+'/'+files[i]  \n",
    "            new_file_path = new_path+ '/'+ files[i]  \n",
    "            count += 1\n",
    "            shutil.copy(file_path,new_file_path)\n",
    "        else:\n",
    "            print(files[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
